{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGxUpCpjDVo5"
   },
   "source": [
    "## Working directory set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lzCAyKNDJa7"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "_drive = False\n",
    "_local = True\n",
    "try: \n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  _drive = True\n",
    "  _local = False\n",
    "except: \n",
    "  print('no google drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iL0F3CjDW7-",
    "outputId": "374cd423-c85b-4d94-9033-b4036b1c2bda"
   },
   "outputs": [],
   "source": [
    "cd drive/MyDrive/PhD/Thesis/Dissertation/Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgAwyZEvDabI",
    "outputId": "df4e9eb0-4485-402b-c00c-202269e7c96b"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DyJhe3bwrZO",
    "outputId": "4342c3d5-d6a8-4266-a1ff-e8f9b2ea584a"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow_addons\n",
    "!pip install imblearn\n",
    "!pip install openpyxl \n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bCrTSpxDbGp"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GybUxhfDgOf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization, LSTM, Conv1D\n",
    "from keras.models import Sequential\n",
    "from dataclasses import dataclass\n",
    "import tensorflow_datasets as tfds \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "import os\n",
    "from   datetime import date\n",
    "\n",
    "from keras import callbacks\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1lJpZfmAiOz"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # preprocessing params\n",
    "    MAX_LEN = 256      \n",
    "    VOCAB_SIZE = 30000  \n",
    "\n",
    "    # model params\n",
    "    EMBED_DIM = 128\n",
    "    FF_DIM = 128  \n",
    "    NUM_LAYERS = 3 \n",
    "\n",
    "    LR = 0.001   \n",
    "    DROPOUT = 0.1 \n",
    "    BATCH_SIZE = 32     \n",
    "\n",
    "    # transformer \n",
    "    NUM_HEAD = 8 \n",
    "\n",
    "    # parallelism\n",
    "    BUFFER_SIZE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    CODE_VERSION = 'Not defined'\n",
    "\n",
    "    def save(self, fname):\n",
    "      dict1 = {'MAX_LEN' : self.MAX_LEN, \n",
    "               'VOCAB_SIZE' : self.VOCAB_SIZE,\n",
    "\n",
    "               'EMBED_DIM' : self.EMBED_DIM,\n",
    "               'BATCH_SIZE' : self.BATCH_SIZE,\n",
    "\n",
    "               'LR' : self.LR,\n",
    "               'DROPOUT' : self.DROPOUT,\n",
    "               'FF_DIM' : self.FF_DIM,\n",
    "               'NUM_LAYERS' : self.NUM_LAYERS,\n",
    "\n",
    "               'CODE_VERSION' : self.CODE_VERSION,\n",
    "               \n",
    "               }\n",
    "      file1 = open(fname, \"w\") \n",
    "      str1 = repr(dict1)\n",
    "      file1.write(\"config = \" + str1 + \"\\n\")\n",
    "      file1.close()\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xp7kx9WQNRnw"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKr8FG_p3l_G"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whaRN8up3qx4"
   },
   "outputs": [],
   "source": [
    "def get_vectorize_layer( vocab_size, max_seq, vec_log='tv_layer.pkl'):\n",
    "\n",
    "    try: \n",
    "      from_disk = pickle.load(open(vec_log, \"rb\"))\n",
    "      vectorize_layer = TextVectorization.from_config(from_disk['config'])\n",
    "      vectorize_layer.set_weights(from_disk['weights'])\n",
    "      print('vectorize layer already existed')\n",
    "\n",
    "    except:\n",
    "      print('vectorize layer not found')\n",
    "\n",
    "      vectorize_layer = TextVectorization(\n",
    "          max_tokens=vocab_size,\n",
    "          output_mode=\"int\",\n",
    "          standardize=None,\n",
    "          output_sequence_length=max_seq,\n",
    "      )\n",
    "      text_ds = tf.data.Dataset.from_tensor_slices(texts).prefetch(config.BUFFER_SIZE)\n",
    "      vectorize_layer.adapt(text_ds)\n",
    "\n",
    "      # Insert mask token in vocabulary\n",
    "      vocab = vectorize_layer.get_vocabulary()\n",
    "      vocab = vocab[2 : vocab_size ]\n",
    "      vectorize_layer.set_vocabulary(vocab)\n",
    "\n",
    "\n",
    "    return vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORPBRyqJ3--3"
   },
   "outputs": [],
   "source": [
    "def encode(texts):\n",
    "    texts = tf.expand_dims(texts, -1)  \n",
    "    encoded_texts = tf.squeeze(vectorize_layer(texts))\n",
    "    return encoded_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YunAiFaPJEDY"
   },
   "source": [
    "### Data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9yjcozoMQrX"
   },
   "outputs": [],
   "source": [
    "def data_balancing(x, y):\n",
    "\n",
    "  ROS = SMOTE(sampling_strategy=1, k_neighbors =3)\n",
    "  X_bal, y_bal = ROS.fit_resample(x, y)\n",
    "\n",
    "  return X_bal, y_bal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7yRkFbfEsGP"
   },
   "source": [
    "## Load labeled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NaXMLZDj4_LN"
   },
   "outputs": [],
   "source": [
    "def split_data(data, training_ratio, _valid=False):\n",
    "\n",
    "  #train_size=0.8\n",
    "\n",
    "  X = data.drop(columns = ['label']).copy()\n",
    "  y = data['label']\n",
    "\n",
    "  X_train, X_rem, y_train, y_rem = train_test_split(X,y,stratify = y, train_size=training_ratio)\n",
    "\n",
    "  if  _valid == False:\n",
    "    return X_train, X_rem, y_train, y_rem\n",
    "  \n",
    "  else:\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
    "    return  X_train, X_valid, X_test, y_train, y_valid, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTWDwAve_XxZ"
   },
   "outputs": [],
   "source": [
    "def load_labeled_dataset(fname, training_ratio=0.7):\n",
    "  dataset = pd.read_csv (fname, index_col =None, header =0)\n",
    "\n",
    "  all_data = dataset[['Code', 'Smelly']]\n",
    "  all_data = all_data.rename(columns={\"Code\": \"code\", \"Smelly\": \"label\"})\n",
    "\n",
    "  train_df = pd.DataFrame()\n",
    "  test_df = pd.DataFrame()\n",
    "  X_train,  X_test, y_train, y_test= split_data(all_data, training_ratio= training_ratio)\n",
    "  train_df = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "  test_df = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "  return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuSRBH8ZEvCR"
   },
   "outputs": [],
   "source": [
    "def get_finetuning_dataset(train_df, test_df):\n",
    "# Prepare datasets for training\n",
    "\n",
    "  x_train = encode(train_df.code.values)\n",
    "  y_train = train_df.label.values\n",
    "\n",
    "  # for testing\n",
    "  x_test = encode(test_df.code.values)\n",
    "  y_test = test_df.label.values\n",
    "\n",
    "  return x_train, y_train, x_test, y_test #train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpkYw-3CMgKc"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(x_train, y_train, x_test, y_test, balance = False):\n",
    "  if balance:\n",
    "    #print(Counter(y_train))\n",
    "    x_train, y_train = data_balancing(x_train, y_train)\n",
    "    print(Counter(y_train))\n",
    "    x_test, y_test = data_balancing( x_test, y_test)\n",
    "    print(Counter(y_test))\n",
    "\n",
    "  return x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gN_4f0SDuSv"
   },
   "source": [
    "## Fine-tuning and Feature-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-x8V-1BCEmQh"
   },
   "outputs": [],
   "source": [
    "def create_classifier_model(pretrained_model):\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "    sequence_output = pretrained_model(inputs)\n",
    "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(pooled_output)\n",
    "    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    classifer_model.compile(\n",
    "        optimizer=optimizer, loss=loss, metrics=metrics\n",
    "    )\n",
    "    return classifer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU3_5dVuOshj"
   },
   "outputs": [],
   "source": [
    "def fine_tune(pretrained_model, x_train, y_train, x_test, y_test):\n",
    "    print('================== Start training ==================\\n')\n",
    "    # Freeze it\n",
    "    pretrained_model.trainable = False\n",
    "    classifer_model = create_classifier_model(pretrained_model)\n",
    "    #classifer_model.summary()\n",
    "\n",
    "    # Train the classifier with frozen  stage (feature-based)\n",
    "    start_time = time.time()\n",
    "    classifer_model.fit(x_train, y_train, epochs=epochs, validation_data=[x_test, y_test ], verbose=0)\n",
    "    train_time_1 = round((time.time() - start_time))\n",
    "    history_1 = classifer_model.evaluate(x_test, y_test)\n",
    "\n",
    "    # Unfreeze the model for fine-tuning\n",
    "    pretrained_model.trainable = True\n",
    "    classifer_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    start_time = time.time()\n",
    "    classifer_model.fit(x_train, y_train, epochs=epochs, validation_data=[x_test, y_test], verbose=0)\n",
    "    train_time_2 = round((time.time() - start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history_2 = classifer_model.evaluate(x_test, y_test)\n",
    "    EVAL_TIME = round((time.time() - start_time))\n",
    "    print('EVAL_TIME = ', EVAL_TIME)\n",
    "\n",
    "    return history_1, history_2, train_time_1, train_time_2, classifer_model, #EVAL_TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjtbH32sR_oO"
   },
   "outputs": [],
   "source": [
    "def fine_tune_feature_based(pretrained_model, x_train, y_train, x_test, y_test):\n",
    "    print('================== Start training ==================\\n')\n",
    "    # Freeze it\n",
    "    pretrained_model.trainable = False\n",
    "    classifer_model = create_classifier_model(pretrained_model)\n",
    "    #classifer_model.summary()\n",
    "\n",
    "    # Train the classifier with frozen  stage\n",
    "    start_time = time.time()\n",
    "    classifer_model.fit(x_train, y_train, epochs=epochs, validation_data=[x_test, y_test ], verbose=0)\n",
    "    train_time_1 = round((time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    history_1 = classifer_model.evaluate(x_test, y_test)\n",
    "    eval_time_1 = round((time.time() - start_time))\n",
    "\n",
    "\n",
    "    return history_1,  eval_time_1, classifer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-fKHRPFH-py"
   },
   "outputs": [],
   "source": [
    "def getToday ():\n",
    "  return str(date.today())\n",
    "\n",
    "def getTime ():\n",
    "  t = time.localtime()\n",
    "  current_time = time.strftime('%H-%M-%S', t)\n",
    "  return current_time\n",
    "\n",
    "def Save_Excel (result, fname, mode='w'):\n",
    "  \n",
    "  if not os.path.isfile(fname) : result.to_csv (fname, header='column_names')\n",
    "  else : \n",
    "    result.to_csv(fname,  mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGy8xjD6V1IN"
   },
   "outputs": [],
   "source": [
    "def mcc(tp,fp,tn,fn):\n",
    "    try:   \n",
    "        return (tp*tn - fp*fn)/((tp+fp)*(tp+fn)*(fn+tn)*(fp+tn))**0.5\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghudLsg88buJ"
   },
   "outputs": [],
   "source": [
    "def get_result(history, time,  proj, m_name, eval_proj=None, n_samples =-1):\n",
    "    \n",
    "    if (n_samples == -1) and (eval_proj==None):\n",
    "        results = pd.DataFrame  (columns = ['Dataset', 'Model', 'Time (Sec)', 'Accuracy', 'Precision', 'Recall',\n",
    "                                            'F1-score', 'AUC','MCC','TP', 'TN', 'FP',  'FN'])\n",
    "        pres = np.round(history[2] * 100,2)\n",
    "        rec = np.round(history[3] * 100,2)\n",
    "\n",
    "        results = results.append({'Dataset'    : proj[0:-4],\n",
    "                            'Model' : m_name,\n",
    "                            'Time (Sec)'       : time,\n",
    "                            'Accuracy'   : np.round(history[1] * 100,2),\n",
    "                            'Precision'  : pres,\n",
    "                            'Recall'     : rec, \n",
    "                            'F1-score'   : np.round( 2* ((pres*rec)/(pres+rec)),2),\n",
    "                            'AUC'        : np.round(history[4] * 100,2),\n",
    "                            'MCC'        : np.round(mcc(history[5],history[7],history[6],history[8]) * 100,2),\n",
    "                            'TP'         : history[5],\n",
    "                            'TN'         : history[6],\n",
    "                            'FP'         : history[7],\n",
    "                            'FN'         : history[8],\n",
    "                            },\n",
    "                            ignore_index = True)\n",
    "    \n",
    "    elif n_samples != -1:\n",
    "        results = pd.DataFrame  (columns = ['Dataset', 'Model', 'Size', 'Time (Sec)', 'Accuracy', 'Precision', 'Recall',\n",
    "                                            'F1-score', 'AUC','MCC','TP', 'TN', 'FP',  'FN'])\n",
    "        pres = np.round(history[2] * 100,2)\n",
    "        rec = np.round(history[3] * 100,2)\n",
    "\n",
    "        results = results.append({'Dataset'    : proj[0:-4],\n",
    "                            'Model' : m_name,\n",
    "                            'Size'  : n_samples,\n",
    "                            'Time (Sec)'       : time,\n",
    "                            'Accuracy'   : np.round(history[1] * 100,2),\n",
    "                            'Precision'  : pres,\n",
    "                            'Recall'     : rec, \n",
    "                            'F1-score'   : np.round( 2* ((pres*rec)/(pres+rec)),2),\n",
    "                            'AUC'        : np.round(history[4] * 100,2),\n",
    "                            'MCC'        : np.round(mcc(history[5],history[7],history[6],history[8]) * 100,2),\n",
    "                            'TP'         : history[5],\n",
    "                            'TN'         : history[6],\n",
    "                            'FP'         : history[7],\n",
    "                            'FN'         : history[8],\n",
    "                            },\n",
    "                            ignore_index = True)\n",
    "    elif eval_proj != None:\n",
    "        results = pd.DataFrame  (columns = ['Training dataset', 'Evaluation dataset', 'Model', 'Time (Sec)', 'Accuracy', 'Precision', 'Recall',\n",
    "                                            'F1-score', 'AUC','MCC','TP', 'TN', 'FP',  'FN'])\n",
    "        pres = np.round(history[2] * 100,2)\n",
    "        rec = np.round(history[3] * 100,2)\n",
    "\n",
    "        results = results.append({'Training dataset'    : proj[0:-4],\n",
    "                            'Evaluation dataset' : eval_proj[0:-4],\n",
    "                            'Model'  : m_name,\n",
    "                            'Time (Sec)'       : time,\n",
    "                            'Accuracy'   : np.round(history[1] * 100,2),\n",
    "                            'Precision'  : pres,\n",
    "                            'Recall'     : rec, \n",
    "                            'F1-score'   : np.round( 2* ((pres*rec)/(pres+rec)),2),\n",
    "                            'AUC'        : np.round(history[4] * 100,2),\n",
    "                            'MCC'        : np.round(mcc(history[5],history[7],history[6],history[8]) * 100,2),\n",
    "                            'TP'         : history[5],\n",
    "                            'TN'         : history[6],\n",
    "                            'FP'         : history[7],\n",
    "                            'FN'         : history[8],\n",
    "                            },\n",
    "                            ignore_index = True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfsKBwprO8tq"
   },
   "source": [
    "## _ Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRukTaOXN8ed",
    "outputId": "e051fb62-8dff-4495-b3f8-eb7a0c4facda"
   },
   "outputs": [],
   "source": [
    "vec_layer_logs = 'vec_layer_logs/'+''+'tv_layer.pkl'\n",
    "\n",
    "# # Get vectorize layer\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "    #train_df.code.values.tolist(),\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN, \n",
    "    vec_log= vec_layer_logs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uw_rKN3JCrHP"
   },
   "outputs": [],
   "source": [
    "# for loops cs>>models>>proj\n",
    "not_completed = []\n",
    "\n",
    "CODE_SMELLS = ['Data class', 'God class', 'Feature envy', 'Long method']\n",
    "\n",
    "DATA_PATH = 'Data/Fine-tuning/Code smells/'\n",
    "SAVE_PATH = 'Results/' + getToday() + '_' + getTime()\n",
    "\n",
    "# Set training parameters \n",
    "optimizer = keras.optimizers.Adam()\n",
    "loss=\"binary_crossentropy\"\n",
    "metrics=['accuracy', tf.metrics.Precision(), tf.metrics.Recall(),  tf.metrics.AUC(),\n",
    "         tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), \n",
    "         tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()]\n",
    "epochs=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xi3hLraR_oP",
    "outputId": "a167af97-3ebc-4c23-c196-21bf5051e15b"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = {'ANN':'logs/hparam_tuning/DL 16-11-2022/run-18/dl_model.h5',\n",
    "              'CNN':'logs/hparam_tuning/CNN 06-12-2022/run-20/cnn_model.h5', \n",
    "              'LSTM': 'logs/hparam_tuning/LSTM 21-11-2022/run-19/lstm_model.h5',\n",
    "              'Transformer':'logs/hparam_tuning/Transformer 15-12-2022/run-6/transformer_model.h5',\n",
    "              }\n",
    "\n",
    "#CODE_SMELLS = ['Long method']\n",
    "\n",
    "m_name = 'Transformer' #item[0]\n",
    "m_best = MODEL_NAME[m_name] #item[1]\n",
    "print(m_name, m_best)\n",
    "\n",
    "# Load pretrained model\n",
    "model_path = m_best #\"transformer_model.h5\"\n",
    "loaded_model = keras.models.load_model(model_path, custom_objects={\"DL_Model\": 'DLModel'})\n",
    "pretrained_model = tf.keras.Model(loaded_model.input, loaded_model.get_layer(index=len(loaded_model.layers)-1).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpN7Nzow5rKn"
   },
   "outputs": [],
   "source": [
    "\n",
    "for code_smell in CODE_SMELLS:\n",
    "    \n",
    "    PROJECTS = os.listdir(DATA_PATH + code_smell +'/') \n",
    "\n",
    "    for proj in PROJECTS:\n",
    "\n",
    "        fname = DATA_PATH + code_smell +'/' + proj\n",
    "        print(fname)\n",
    "\n",
    "        try:\n",
    "\n",
    "            train_df, test_df= load_labeled_dataset(fname)\n",
    "            x_train, y_train, x_test, y_test =  get_finetuning_dataset(train_df, test_df)\n",
    "            x_train, y_train, x_test, y_test =  data_preprocessing(x_train, y_train, x_test, y_test, balance=True)\n",
    "\n",
    "\n",
    "        except:\n",
    "            print('Data has less than 6 positive labels: ', fname)\n",
    "            continue\n",
    "        \n",
    "\n",
    "        try:\n",
    "              # fine tuning \n",
    "                \n",
    "              history_1, history_2, train_time_1, train_time_2, _ = fine_tune(pretrained_model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "              print(history_2)\n",
    "\n",
    "              # Save results\n",
    "\n",
    "              fine_tuning_results =  get_result(history_2, train_time_2,  proj, m_name)\n",
    "              feature_based_results = get_result(history_1, train_time_1,  proj, m_name)\n",
    "\n",
    "              Save_Excel (fine_tuning_results,  SAVE_PATH + '_' + code_smell+ '_FineTuningResults_SMOTE_'+m_name+'.csv')\n",
    "              Save_Excel (feature_based_results,  SAVE_PATH +'_' + code_smell+ '_FeatureBasedResults_SMOTE_'+m_name+'.csv')\n",
    "\n",
    "        except Exception as e: \n",
    "              print(e)\n",
    "              not_completed.append((code_smell, proj))\n",
    "\n",
    "print(not_completed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUHMSOtLEII8"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgggBLqV9lKe"
   },
   "source": [
    "### Unseen projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sdie4DpER_oQ"
   },
   "outputs": [],
   "source": [
    "def cross_proj_evaluation(model, m_name, file_path, train_proj, eval_projects):\n",
    "  results = pd.DataFrame()\n",
    "  for proj in eval_projects:\n",
    "    fname = file_path+ proj\n",
    "    print('-------------- Evaluation on ---------------\\n', fname)\n",
    "\n",
    "    try:\n",
    "        train_df, _ = load_labeled_dataset(fname, training_ratio=0.99)\n",
    "        x_train, y_train, _, _ =  get_finetuning_dataset(train_df, train_df)\n",
    "        _, _, x_test, y_test =  data_preprocessing(x_train, y_train, x_train, y_train, balance=True)\n",
    "\n",
    "    except  Exception as e: \n",
    "        print(e)\n",
    "        print('Data has less than 6 positive labels: ', fname)\n",
    "        continue\n",
    "    start_time = time.time()\n",
    "    history = model.evaluate(x_test, y_test)\n",
    "    train_time = round((time.time() - start_time))\n",
    "    \n",
    "    fine_tuning_results =  get_result(history, train_time,  train_proj , m_name, eval_proj=proj)\n",
    "    results = results.append(fine_tuning_results)\n",
    "\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4rVu00NR_oQ"
   },
   "outputs": [],
   "source": [
    "def get_evaluation_projects(code_smell):\n",
    "\n",
    "  eval_projects = pd.read_csv ('cross-projects.csv', index_col =None, header =0)\n",
    "  eval_projects = pd.unique(eval_projects[code_smell])\n",
    "  eval_projects =  [x+'.csv' for x in eval_projects if str(x) != 'nan']\n",
    "\n",
    "  return eval_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXcMl-o0R_oQ",
    "outputId": "84fb59bc-67a1-4101-eba5-74417ef7d063"
   },
   "outputs": [],
   "source": [
    "for code_smell in CODE_SMELLS:\n",
    "    \n",
    "    PROJECTS = get_evaluation_projects(code_smell)\n",
    "\n",
    "    for proj in PROJECTS:\n",
    "\n",
    "        fname = DATA_PATH + code_smell +'/' + proj \n",
    "        print(fname)\n",
    "\n",
    "        try:\n",
    "\n",
    "            train_df, test_df= load_labeled_dataset(fname)\n",
    "            x_train, y_train, x_test, y_test =  get_finetuning_dataset(train_df, test_df)\n",
    "            x_train, y_train, x_test, y_test =  data_preprocessing(x_train, y_train, x_test, y_test, balance=True)\n",
    "\n",
    "        except:\n",
    "            print('Data has less than 6 positive labels: ', fname)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "          # fine tuning \n",
    "\n",
    "          #history_1, history_2, train_time_1, train_time_2, model = fine_tune(pretrained_model, x_train, y_train, x_test, y_test)\n",
    "          history_1, eval_time_1, model = fine_tune_feature_based(pretrained_model, x_train, y_train, x_test, y_test)\n",
    "          #print(history_2)\n",
    "\n",
    "          # Save results\n",
    "\n",
    "          #fine_tuning_results =  get_result(history_2, 0,  proj, m_name, eval_proj = proj)\n",
    "          #cross_proj_results =  cross_proj_evaluation(model, m_name, DATA_PATH + code_smell +'/' , proj , sorted(set(PROJECTS)-set([proj])))\n",
    "          #fine_tuning_results = fine_tuning_results.append(cross_proj_results)      \n",
    "         \n",
    "          feature_based_results = get_result(history_1, eval_time_1,  proj, m_name, eval_proj = proj)\n",
    "          cross_proj_results =  cross_proj_evaluation(model, m_name, DATA_PATH + code_smell +'/' , proj , sorted(set(PROJECTS)-set([proj])))\n",
    "          feature_based_results = feature_based_results.append(cross_proj_results)  \n",
    "          \n",
    "          #Save_Excel (fine_tuning_results,  SAVE_PATH + '_' + code_smell+ '_FineTuningResults_CrossProj_'+m_name+'.csv')\n",
    "          Save_Excel (feature_based_results,  SAVE_PATH +'_' + code_smell+ '_FeatureBasedResults_CrossProj_'+m_name+'.csv')\n",
    "\n",
    "        except Exception as e: \n",
    "          print(e)\n",
    "          not_completed.append((code_smell, proj))\n",
    "\n",
    "print(not_completed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqPYYQcF9PUB"
   },
   "source": [
    "### Dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUt3bK0c7Fup"
   },
   "outputs": [],
   "source": [
    "def load_labeled_dataset_with_size(fname, n_samples):\n",
    "  dataset = pd.read_csv (fname, index_col =None, header =0)\n",
    "\n",
    "  all_data = dataset[['Code', 'Smelly']]\n",
    "  all_data = all_data.rename(columns={\"Code\": \"code\", \"Smelly\": \"label\"})\n",
    "  \n",
    "  all_data = all_data.sample(n=n_samples, random_state=1).reset_index(drop=True)#.sort_values('label', ascending =False)#\n",
    "\n",
    "  X_train,  X_test, y_train, y_test= split_data(all_data, training_ratio= 0.7)\n",
    "\n",
    "  train_df = pd.DataFrame()\n",
    "  test_df = pd.DataFrame()\n",
    "\n",
    "  train_df = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "  test_df = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "  return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggicjvunBWux"
   },
   "outputs": [],
   "source": [
    "# define dataset sizes to evaluate\n",
    "sizes = [100, 500, 1000, 1500]\n",
    "score_sets, means = list(), list()\n",
    "not_completed = []\n",
    "\n",
    "for code_smell in CODE_SMELLS:\n",
    "    \n",
    "    PROJECTS = os.listdir(DATA_PATH + code_smell +'/') \n",
    "    #PROJECTS = ['accumulo.csv', 'airavata.csv',]\n",
    "\n",
    "    for proj in PROJECTS:\n",
    "\n",
    "        fname = DATA_PATH + code_smell +'/' + proj\n",
    "        print(fname)\n",
    "        for n_samples in sizes: \n",
    "          try:\n",
    "\n",
    "              train_df, test_df = load_labeled_dataset_with_size(fname, n_samples)\n",
    "              x_train, y_train, x_test, y_test =  get_finetuning_dataset(train_df, test_df)\n",
    "              x_train, y_train, x_test, y_test =  data_preprocessing(x_train, y_train, x_test, y_test, balance=True)\n",
    "\n",
    "          except:\n",
    "              print('Data has less than 6 positive labels: ', fname)\n",
    "              break#continue\n",
    "\n",
    "          try:\n",
    "            x=1\n",
    "            # fine tuning \n",
    "            history_1, history_2, train_time_1, train_time_2, _ = fine_tune(pretrained_model, x_train, y_train, x_test, y_test)\n",
    "            #print(history_2)\n",
    "            #history_1, train_time_1, model = baseline_model( x_train, y_train, x_test, y_test)\n",
    "\n",
    "            # Save results\n",
    "\n",
    "            dataset_size_tuning_results =  get_result(history_2, train_time_2,  proj, m_name, None, n_samples)\n",
    "            dataset_size_feature_results = get_result(history_1, train_time_1,  proj, m_name, None, n_samples)\n",
    "\n",
    "            Save_Excel (dataset_size_tuning_results,  SAVE_PATH + '_' + code_smell+ '_FineTuningResults_Size_'+m_name+'.csv')\n",
    "            Save_Excel (dataset_size_feature_results,  SAVE_PATH +'_' + code_smell+ '_FeatureBasedResults_Size_'+m_name+'.csv')\n",
    "\n",
    "          except:\n",
    "            x=6\n",
    "            not_completed.append((code_smell, proj))\n",
    "          \n",
    "          print('Size=%d: %s' % (n_samples,fname))\n",
    "\n",
    "print(not_completed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gk0KWA-GR_oQ"
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYcROArmR_oR"
   },
   "outputs": [],
   "source": [
    "def baseline_model( x_train, y_train, x_test, y_test):\n",
    "\n",
    "  regression_layer = keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "  inputs = keras.Input(shape=(config.MAX_LEN))\n",
    "  outputs = regression_layer(inputs)\n",
    "  baseline_model = keras.Model(inputs, outputs)\n",
    "  baseline_model.compile(loss=loss, metrics=metrics)\n",
    "\n",
    "  start_time = time.time()\n",
    "  baseline_model.fit(x_train, y_train, epochs=epochs, validation_data=[x_test, y_test ], verbose=0)\n",
    "  train_time_1 = round((time.time() - start_time))\n",
    "  history_1 = baseline_model.evaluate(x_test, y_test)\n",
    "\n",
    "  return history_1,train_time_1, baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E352dnJLR_oR"
   },
   "outputs": [],
   "source": [
    "m_name = 'Baseline'\n",
    "not_completed=[]\n",
    "for code_smell in CODE_SMELLS:\n",
    "    \n",
    "    #PROJECTS = os.listdir(DATA_PATH + code_smell +'/') \n",
    "    #PROJECTS = ['activemq.csv', 'airavata.csv',]\n",
    "    PROJECTS = get_evaluation_projects(code_smell)\n",
    "\n",
    "    for proj in PROJECTS:\n",
    "\n",
    "        fname = DATA_PATH + code_smell +'/' + proj\n",
    "        print(fname)\n",
    "\n",
    "        try:\n",
    "\n",
    "            train_df, test_df= load_labeled_dataset(fname)\n",
    "            x_train, y_train, x_test, y_test =  get_finetuning_dataset(train_df, test_df)\n",
    "            x_train, y_train, x_test, y_test =  data_preprocessing(x_train, y_train, x_test, y_test, balance=True)\n",
    "\n",
    "\n",
    "        except:\n",
    "            print('Data has less than 6 positive labels: ', fname)\n",
    "            continue\n",
    "\n",
    "\n",
    "        try:\n",
    "          # fine tuning \n",
    "          history_1, train_time_1, model = baseline_model( x_train, y_train, x_test, y_test)\n",
    "\n",
    "          # Save results\n",
    "          baseline_results = get_result(history_1, train_time_1,  proj, m_name)\n",
    "          Save_Excel (baseline_results,  SAVE_PATH +'_' + code_smell+ '_BaselineResults.csv')\n",
    "        \n",
    "          \n",
    "          #cross projects evaluation\n",
    "          baseline_results = get_result(history_1, train_time_1,  proj, m_name, eval_proj = proj)\n",
    "          cross_proj_results =  cross_proj_evaluation(model, m_name, DATA_PATH + code_smell +'/' , proj , sorted(set(PROJECTS)-set([proj])))\n",
    "          baseline_results = baseline_results.append(cross_proj_results)  \n",
    "          Save_Excel (baseline_results,  SAVE_PATH +'_' + code_smell+ '_BaselineResults_CrossProj_'+m_name+'.csv')\n",
    "\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "          not_completed.append((code_smell, proj))\n",
    "\n",
    "print(not_completed)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
