{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5vBuvW1uTIM"
   },
   "source": [
    "## Working directory set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SxGR_avkFteZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no google drive\n"
     ]
    }
   ],
   "source": [
    "_drive = False\n",
    "_local = True\n",
    "try: \n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  _drive = True\n",
    "  _local = False\n",
    "except: \n",
    "  print('no google drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwDA3Bb4GBGd"
   },
   "outputs": [],
   "source": [
    "cd drive/MyDrive/PhD/Thesis/Dissertation/Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjgtzsnXjXbx"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vDWAmcpMc8zn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: 第 1 行： [: 缺少 \"]\"\n",
      "Directory created\n",
      "/bin/bash: 第 1 行： [: 缺少 \"]\"\n",
      "mkdir: 无法创建目录 \"logs/optuna\": 没有那个文件或目录\n",
      "/bin/bash: 第 1 行： [: 缺少 \"]\"\n",
      "Directory created\n",
      "/bin/bash: 第 1 行： [: 缺少 \"]\"\n",
      "Directory created\n",
      "/bin/bash: 第 1 行： [: 缺少 \"]\"\n",
      "Directory created\n",
      "/bin/bash: 第 1 行： [: 缺少 \"]\"\n",
      "Directory created\n",
      "/bin/bash: 第 1 行： [: 缺少 \"]\"\n",
      "Directory created\n"
     ]
    }
   ],
   "source": [
    "!if [ -d 'vec_layer_logs']; then echo \"Directory already exist\" ; else mkdir 'vec_layer_logs'   && echo \"Directory created\"; fi\n",
    "!if [ -d 'logs/optuna']; then echo \"Directory already exist\" ; else mkdir 'logs/optuna'   && echo \"Directory created\"; fi\n",
    "!if [ -d 'Preprocessed datasets/']; then echo \"Directory already exist\" ; else mkdir 'Preprocessed datasets/'   && echo \"Directory created\"; fi\n",
    "!if [ -d 'Preprocessed datasets/pre-training/']; then echo \"Directory already exist\" ; else mkdir 'Preprocessed datasets/pre-training/'   && echo \"Directory created\"; fi\n",
    "!if [ -d 'Preprocessed datasets/pre-training/training/']; then echo \"Directory already exist\" ; else mkdir 'Preprocessed datasets/pre-training/training/'   && echo \"Directory created\"; fi\n",
    "!if [ -d 'Preprocessed datasets/pre-training/validation/']; then echo \"Directory already exist\" ; else mkdir 'Preprocessed datasets/pre-training/validation/'   && echo \"Directory created\"; fi\n",
    "!if [ -d 'Preprocessed datasets/pre-training/testing/']; then echo \"Directory already exist\" ; else mkdir 'Preprocessed datasets/pre-training/testing/'   && echo \"Directory created\"; fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vO6qFQJz-st3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting tensorboard_plugin_profile\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4f/82/3d4ad87691a53cdcc902d6b44a2565fa96354a953125597146800d9b52c2/tensorboard_plugin_profile-2.19.8-cp310-none-manylinux2014_x86_64.whl (12.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting gviz_api>=1.9.0 (from tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/67/42/e6ae4f7903f17be07c47b7af1f6d83ec4fe931f373f900f542d737d9940e/gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting protobuf>=3.19.6 (from tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fa/b1/b59d405d64d31999244643d88c45c8241c58f17cc887e73bcb90602327f8/protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/bdww/anaconda3/envs/lxj_LLM/lib/python3.10/site-packages (from tensorboard_plugin_profile) (80.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/bdww/.local/lib/python3.10/site-packages (from tensorboard_plugin_profile) (1.16.0)\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/52/24/ab44c871b0f07f491e5d2ad12c9bd7358e527510618cb1b803a88e986db1/werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Collecting etils>=1.0.0 (from etils[epath]>=1.0.0->tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/dd/71/40ee142e564b8a34a7ae9546e99e665e0001011a3254d5bbbe113d72ccba/etils-1.12.2-py3-none-any.whl (167 kB)\n",
      "Collecting cheroot>=10.0.1 (from tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5b/9e/111f4441bc0e4736251ee0e61094904432ab19292a4cc7bd7127ed94f81e/cheroot-10.0.1-py3-none-any.whl (104 kB)\n",
      "Requirement already satisfied: fsspec>=2024.10.0 in /home/bdww/anaconda3/envs/lxj_LLM/lib/python3.10/site-packages (from fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (2025.3.0)\n",
      "Collecting more-itertools>=2.6 (from cheroot>=10.0.1->tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2b/9f/7ba6f94fc1e9ac3d2b853fdff3035fb2fa5afbed898c4a72b8a020610594/more_itertools-10.7.0-py3-none-any.whl (65 kB)\n",
      "Collecting jaraco.functools (from cheroot>=10.0.1->tensorboard_plugin_profile)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/9f/4f/24b319316142c44283d7540e76c7b5a6dbd5db623abd86bb7b3491c21018/jaraco.functools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: importlib_resources in /home/bdww/.local/lib/python3.10/site-packages (from etils[epath]>=1.0.0->tensorboard_plugin_profile) (5.12.0)\n",
      "Requirement already satisfied: typing_extensions in /home/bdww/.local/lib/python3.10/site-packages (from etils[epath]>=1.0.0->tensorboard_plugin_profile) (4.12.2)\n",
      "Requirement already satisfied: zipp in /home/bdww/.local/lib/python3.10/site-packages (from etils[epath]>=1.0.0->tensorboard_plugin_profile) (3.20.2)\n",
      "Collecting gcsfs (from fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ff/eb/9182e875592c48d282c5eab602000f0618817b9011b2b2925165e4b4b7f3/gcsfs-2025.5.1-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/bdww/.local/lib/python3.10/site-packages (from werkzeug>=0.11.15->tensorboard_plugin_profile) (2.1.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/bdww/anaconda3/envs/lxj_LLM/lib/python3.10/site-packages (from gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (3.11.18)\n",
      "Requirement already satisfied: decorator>4.1.2 in /home/bdww/anaconda3/envs/lxj_LLM/lib/python3.10/site-packages (from gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (5.2.1)\n",
      "INFO: pip is looking at multiple versions of gcsfs to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3d/33/a1e020d25a22eb1565c8304789ba51c4ff0e6d915068a910afbc4780933e/gcsfs-2025.5.0.post1-py2.py3-none-any.whl (36 kB)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/54/dc/6a770adcc6c14cff72ec116d4e620c9485d7d7064a4c4a5135cc2329eb72/gcsfs-2025.5.0-py2.py3-none-any.whl (36 kB)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/30/62/49c6ab9aab52ef66cce230bb130cbd92073f5ffded710c190aca3648d700/gcsfs-2025.3.2-py2.py3-none-any.whl (36 kB)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e0/e2/ca34fec0fdf0ee82277bf5e6862f438a27ed5515c414de601b69138bc637/gcsfs-2025.3.1-py2.py3-none-any.whl (36 kB)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/44/dd/874223310565a336820a70727b61e7dd23f7be6cb91006f2cbb634670142/gcsfs-2025.3.0-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: google-auth>=1.2 in /home/bdww/.local/lib/python3.10/site-packages (from gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (2.35.0)\n",
      "Collecting google-auth-oauthlib (from gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ac/84/40ee070be95771acd2f4418981edb834979424565c3eec3cd88b6aa09d24/google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\n",
      "Collecting google-cloud-storage (from gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/13/b8/c99c965659f45efa73080477c49ffddf7b9aecb00806be8422560bb5b824/google_cloud_storage-3.1.0-py2.py3-none-any.whl (174 kB)\n",
      "Requirement already satisfied: requests in /home/bdww/.local/lib/python3.10/site-packages (from gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (2.32.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/bdww/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/bdww/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/bdww/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/bdww/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/bdww/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/bdww/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/bdww/anaconda3/envs/lxj_LLM/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/bdww/anaconda3/envs/lxj_LLM/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/bdww/.local/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/bdww/.local/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (5.5.0)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.2->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/47/8d/d529b5d697919ba8c11ad626e835d4039be708a35b0d22de83a269a6682c/pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/bdww/.local/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (4.9)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.2->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c8/f1/d6a797abb14f6283c0ddff96bbdd46937f64122b8c925cab503dd37f8214/pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/bdww/.local/lib/python3.10/site-packages (from google-auth-oauthlib->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/bdww/.local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (3.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bdww/.local/lib/python3.10/site-packages (from requests->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bdww/.local/lib/python3.10/site-packages (from requests->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bdww/.local/lib/python3.10/site-packages (from requests->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (2023.11.17)\n",
      "Collecting google-api-core<3.0.0dev,>=2.15.0 (from google-cloud-storage->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ac/ca/149e41a277bb0855e8ded85fd7579d7747c1223e253d82c5c0f1be236875/google_api_core-2.25.0-py3-none-any.whl (160 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.4.2 (from google-cloud-storage->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/40/86/bda7241a8da2d28a754aad2ba0f6776e35b67e37c36ae0c45d49370f1014/google_cloud_core-2.4.3-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media>=2.7.2 (from google-cloud-storage->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/82/35/b8d3baf8c46695858cb9d8835a53baa1eeb9906ddaf2f728a5f5b640fd1e/google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/60/44/87e77e8476767a4a93f6cf271157c6d948eacec63688c093580af13b04be/google_crc32c-1.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/bdww/.local/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile) (1.65.0)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->fsspec[gcs]>=2024.10.0->tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4e/6d/280c4c2ce28b1593a19ad5239c8b826871fc6ec275c21afc8e1820108039/proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Collecting protobuf>=3.19.6 (from tensorboard_plugin_profile)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/85/e4/07c80521879c2d15f321465ac24c70efe2381378c00bf5e56a0f4fbac8cd/protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Installing collected packages: werkzeug, pyasn1, protobuf, more-itertools, gviz_api, google-crc32c, etils, pyasn1-modules, proto-plus, jaraco.functools, google-resumable-media, cheroot, google-auth-oauthlib, google-api-core, google-cloud-core, google-cloud-storage, gcsfs, tensorboard_plugin_profile\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/18\u001b[0m [tensorboard_plugin_profile]rd_plugin_profile]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-proto 1.27.0 requires protobuf<5.0,>=3.19, but you have protobuf 5.29.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cheroot-10.0.1 etils-1.12.2 gcsfs-2025.3.0 google-api-core-2.25.0 google-auth-oauthlib-1.2.2 google-cloud-core-2.4.3 google-cloud-storage-3.1.0 google-crc32c-1.7.1 google-resumable-media-2.7.2 gviz_api-1.10.0 jaraco.functools-4.1.0 more-itertools-10.7.0 proto-plus-1.26.1 protobuf-5.29.5 pyasn1-0.6.1 pyasn1-modules-0.4.2 tensorboard_plugin_profile-2.19.8 werkzeug-3.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U tensorboard_plugin_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UdLkzehmBtR-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting tensorflow\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2b/b6/86f99528b3edca3c31cad43e79b15debc9124c7cbc772a8f8e82667fd427/tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.8/644.8 MB\u001b[0m \u001b[31m728.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:18\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/87/04/9d75e1d3bb4ab8ec67ff10919476ccdee06c098bcfcf3a352da5f985171d/absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/bdww/.local/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/bdww/.local/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/1d/fc/716c1e62e512ef1c160e7984a73a5fc7df45166f2ff3f254e71c58076f7c/libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/23/cd/066e86230ae37ed0be70aae89aabf03ca8d9f39c8aea0dec8029455b5540/opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Requirement already satisfied: packaging in /home/bdww/.local/lib/python3.10/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/bdww/anaconda3/envs/lxj_LLM/lib/python3.10/site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/bdww/.local/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/bdww/anaconda3/envs/lxj_LLM/lib/python3.10/site-packages (from tensorflow) (80.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/bdww/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/bdww/.local/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/bdww/.local/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/bdww/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/bdww/.local/lib/python3.10/site-packages (from tensorflow) (1.66.1)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/5d/12/4f70e8e2ba0dbe72ea978429d8530b0333f0ed2140cc571a48802878ef99/tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/95/e6/4179c461a5fc43e3736880f64dbdc9b1a5349649f0ae32ded927c0e3a227/keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.2.0,>=1.26.0 in /home/bdww/anaconda3/envs/lxj_LLM/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d8/fa/0b6a59a1043c53d5d287effa02303bd248905ee82b25143c7caad8b340ad/h5py-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/16/d8/4502e12c6a10d42e13a552e8d97f20198e3cf82a0d1411ad50be56a5077c/ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f3/48/47b7d25572961a48b1de3729b7a11e835b888e41e0203cca82df95d23b91/tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/bdww/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bdww/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bdww/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bdww/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/bdww/.local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/bdww/anaconda3/envs/lxj_LLM/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/bdww/anaconda3/envs/lxj_LLM/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/bdww/.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b2/bc/465daf1de06409cdd4532082806770ee0d8d7df434da79c76564d0f69741/namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: optree in /home/bdww/anaconda3/envs/lxj_LLM/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/bdww/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/bdww/.local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/bdww/.local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/bdww/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Installing collected packages: namex, libclang, tensorflow-io-gcs-filesystem, tensorboard-data-server, opt-einsum, ml-dtypes, h5py, google-pasta, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [tensorflow]3\u001b[0m [tensorflow]]]ata-server]stem]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.0 astunparse-1.6.3 google-pasta-0.2.0 h5py-3.13.0 keras-3.10.0 libclang-18.1.1 ml-dtypes-0.5.1 namex-0.1.0 opt-einsum-3.4.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex4tgklTq13l"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bn097Sepq13l"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization, LSTM, Conv1D\n",
    "from keras.models import Sequential\n",
    "from dataclasses import dataclass\n",
    "import tensorflow_datasets as tfds \n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "from keras import callbacks\n",
    "\n",
    "# HP tuning \n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import optuna\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# visualization \n",
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_edf\n",
    "from optuna.visualization import plot_intermediate_values\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lktgnJxuf_Jk"
   },
   "outputs": [],
   "source": [
    "print(\"tf version:\",tf.__version__)\n",
    "print(\"keras version:\", keras.__version__)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"numpy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxNS2dYkq13m"
   },
   "source": [
    "## Set-up Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwRCuOwLq13m"
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "  \n",
    "    # preprocessing params\n",
    "    MAX_LEN = 256       #512 in bert\n",
    "    VOCAB_SIZE = 30000  # 30000 in bert     \n",
    "\n",
    "    # mask  15% in bert\n",
    "    KW_MASK_RATE = 0.25\n",
    "    RAND_MASK_RATE = 0.10\n",
    "\n",
    "    # model params\n",
    "    EMBED_DIM = 128\n",
    "    FF_DIM = 2048  # number of units 768 in bert\n",
    "    NUM_LAYERS = 6 # 3, 6, 12 in bert\n",
    "\n",
    "    LR = 0.001   # 0.0001 in bert\n",
    "    DROPOUT = 0.3 # 0.1 in bert\n",
    "    BATCH_SIZE = 128     #256 in bert\n",
    "\n",
    "    # transformer \n",
    "    NUM_HEAD = 8 # 8,16 in atten, 12 in bert\n",
    "\n",
    "    # parallelism\n",
    "    BUFFER_SIZE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    CODE_VERSION = 'Not defined'\n",
    "\n",
    "    def save(self, fname):\n",
    "      dict1 = {'MAX_LEN' : self.MAX_LEN, \n",
    "               'VOCAB_SIZE' : self.VOCAB_SIZE,\n",
    "               'EMBED_DIM' : self.EMBED_DIM,\n",
    "               'BATCH_SIZE' : self.BATCH_SIZE,\n",
    "               'KW_MASK_RATE' : self.KW_MASK_RATE,\n",
    "               'RAND_MASK_RATE' : self.RAND_MASK_RATE,\n",
    "               'LR' : self.LR,\n",
    "               'DROPOUT' : self.DROPOUT,\n",
    "               'FF_DIM' : self.FF_DIM,\n",
    "               'NUM_LAYERS' : self.NUM_LAYERS,\n",
    "               'NUM_HEAD' : self.NUM_HEAD,\n",
    "               'CODE_VERSION' : self.CODE_VERSION,\n",
    "               \n",
    "               }\n",
    "      file1 = open(fname, \"w\") \n",
    "      str1 = repr(dict1)\n",
    "      file1.write(\"config = \" + str1 + \"\\n\")\n",
    "      file1.close()\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9Ds23X2q13m"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-nnPCKTuM91"
   },
   "outputs": [],
   "source": [
    "def load_data_from_file(fpath):\n",
    "  projects_names = pd.read_csv(fpath)\n",
    "  projects_names = projects_names['Project name'].str.split('/',  expand=True)\n",
    "  projects_names.drop(0, inplace=True, axis=1)\n",
    "\n",
    "  fnames = projects_names[1].values.tolist()\n",
    "\n",
    "  dataset = pd.DataFrame()\n",
    "  df = map(lambda f: pd.read_csv ('Data/'+f+'_classes.csv', index_col =None, header =0), fnames)\n",
    "  df = list(df)\n",
    "  dataset = pd.concat(df, axis=0, ignore_index =True) \n",
    "  dataset.rename(columns = {'Class':'code'}, inplace = True)\n",
    "  \n",
    "  dataset['label'] = 1\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buhoZWpOW-s2"
   },
   "outputs": [],
   "source": [
    "def get_keywords(fpath):\n",
    "  keywords_fname = 'Java-keyword.txt'\n",
    "  keywords = [line.rstrip() for line in open(keywords_fname)]\n",
    "  return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DouFv8vql-D"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Df5Jtgoj8jww"
   },
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iNnipW4qovT"
   },
   "outputs": [],
   "source": [
    "def data_filter(data):\n",
    "  fdata = pd.DataFrame([x for a, x in data.iterrows() if len(x[0].split(' ')) > 50]).reset_index(drop =True)\n",
    "  return fdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNnciuXvMn7n"
   },
   "source": [
    "### Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jD88L1tMoLP"
   },
   "outputs": [],
   "source": [
    "def truncateText(text ,max_len):\n",
    "\n",
    "    cursor = 0  \n",
    "    end = max_len\n",
    "    lst = []\n",
    "    text = text.split(' ')\n",
    "\n",
    "    while (end < len(text)):\n",
    "      substr = text[cursor : end]\n",
    "      lst.append(' '.join(substr))\n",
    "      cursor =  end        \n",
    "      end = (cursor+max_len) if (cursor+max_len<len(text)) else len(text)        \n",
    "\n",
    "    if (end-cursor) > (max_len/2): \n",
    "      lst.append(' '.join(text[cursor : end]))\n",
    "\n",
    "    return lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYx2-SmTOFlO"
   },
   "outputs": [],
   "source": [
    "def truncation(data):\n",
    "  fdata = pd.DataFrame()\n",
    "  listdict = []\n",
    "\n",
    "  for a, text in data.iterrows():\n",
    "    t_text = truncateText(text[0], config.MAX_LEN) \n",
    "    listdict.append(pd.DataFrame(data = {'code':t_text, 'label': text[1]}))\n",
    "  \n",
    "  fdata = pd.concat(list(listdict), axis=0, ignore_index =True) \n",
    "\n",
    "  return fdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awqUKOOE8gTA"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbpydcsU8e2L"
   },
   "outputs": [],
   "source": [
    "def get_vectorize_layer(texts, vocab_size, max_seq, vec_log='vec_layer_logs/tv_layer.pkl', special_tokens=[\"[mask]\"]):\n",
    "\n",
    "    try: \n",
    "      from_disk = pickle.load(open(vec_log, \"rb\"))\n",
    "      vectorize_layer = TextVectorization.from_config(from_disk['config'])\n",
    "      vectorize_layer.set_weights(from_disk['weights'])\n",
    "      print('vectorize layer already existed')\n",
    "\n",
    "    except:\n",
    "      print('vectorize layer not found')\n",
    "\n",
    "      vectorize_layer = TextVectorization(\n",
    "          max_tokens=vocab_size,\n",
    "          output_mode=\"int\",\n",
    "          standardize=None,\n",
    "          output_sequence_length=max_seq,\n",
    "      )\n",
    "      text_ds = tf.data.Dataset.from_tensor_slices(texts).prefetch(config.BUFFER_SIZE)\n",
    "      vectorize_layer.adapt(text_ds)\n",
    "\n",
    "      # Insert mask token in vocabulary\n",
    "      vocab = vectorize_layer.get_vocabulary()\n",
    "      vocab = vocab[2 : vocab_size - len(special_tokens)] + special_tokens\n",
    "      vectorize_layer.set_vocabulary(vocab)\n",
    "\n",
    "      # Pickle the config and weights\n",
    "      pickle.dump({'config': vectorize_layer.get_config(),\n",
    "              'weights': vectorize_layer.get_weights()}\n",
    "              , open(vec_log, \"wb\"))\n",
    "\n",
    "    return vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwSCOou6A-8F"
   },
   "outputs": [],
   "source": [
    "def encode(texts):\n",
    "    texts = tf.expand_dims(texts, -1)  \n",
    "    encoded_texts = tf.squeeze(vectorize_layer(texts))\n",
    "    return encoded_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-P2pP28q13n"
   },
   "source": [
    "## Proxytask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SN_A-Sbl0BC4"
   },
   "outputs": [],
   "source": [
    "def get_keywords_pos(text, keywords_dic):\n",
    "  keywors_pos_list = [] \n",
    "\n",
    "  for i in range(text.shape[0]):\n",
    "    temp = []\n",
    "    # get the position of keywords\n",
    "    for a in range(0, len(keywords)):\n",
    "      temp.append(np.flatnonzero(text[i] == keywords_dic[keywords[a]]))\n",
    "\n",
    "    #combine multiple np.arrays to one, return array of np.arrays \n",
    "    keywors_pos_list.append(np.concatenate(temp))\n",
    "\n",
    "  return keywors_pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqCOvfRRxj_F"
   },
   "outputs": [],
   "source": [
    "def get_masked_input_and_labels(encoded_txt):\n",
    "    \n",
    "    encoded_texts = []\n",
    "    for row in encoded_txt:\n",
    "        encoded_texts.append(row.tolist())\n",
    "\n",
    "    encoded_texts = np.array(encoded_texts, int)\n",
    "\n",
    "    #texts_masked = np.copy(encoded_texts)\n",
    "    texts_masked =  np.copy(encoded_texts)\n",
    "    label_list =  np.copy(encoded_texts)\n",
    "\n",
    "    #Num of instances\n",
    "    _m = encoded_texts.shape[0]\n",
    "\n",
    "    # Get keywords posotions list \n",
    "    keywors_pos_list = get_keywords_pos(encoded_texts, keywords_dic)\n",
    "    #print(keywors_pos_list, MASK_TOKEN_ID)\n",
    "\n",
    "    inp_mask = []\n",
    "    for i in range(np.shape(keywors_pos_list)[0]):\n",
    "      # Create random mask on keywords\n",
    "      rand = (np.random.rand(np.shape(keywors_pos_list[i])[0]) < config.KW_MASK_RATE)\n",
    "      inp_mask.append(rand) \n",
    "\n",
    "\n",
    "    selection = []\n",
    "    for i in range(_m):\n",
    "        selection.append(keywors_pos_list[i][((inp_mask[i] == True))])      \n",
    "\n",
    "    # Apply the maske by replacing the selected tokens with MASK_TOKEN_ID\n",
    "    for i in range(_m):\n",
    "      texts_masked[i, selection[i]] = MASK_TOKEN_ID\n",
    "\n",
    "\n",
    "    # Randomly mask 10% of the text\n",
    "    inp_mask_2 =  (np.random.rand(*encoded_texts.shape) <  config.RAND_MASK_RATE)\n",
    "    # Do not mask special tokens \n",
    "    inp_mask_2[encoded_texts <= 2] = False\n",
    "    # Apply the mask\n",
    "    texts_masked[inp_mask_2] = MASK_TOKEN_ID \n",
    "    \n",
    "    # Make keywords mask to true\n",
    "    inp_mask_2[texts_masked == MASK_TOKEN_ID] = True\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask_2] = texts_masked[inp_mask_2]\n",
    "\n",
    "    texts_masked = np.array(texts_masked, int)\n",
    "    labels = np.array(labels, int)\n",
    "\n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "\n",
    "    return texts_masked, label_list, sample_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Er2BJGgny0i_"
   },
   "source": [
    "## Prepare datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thKjW7VMzQIo"
   },
   "source": [
    "### Pre-training DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEKbIHdSnUUO"
   },
   "outputs": [],
   "source": [
    "def get_pretraining_dataset(X_train, X_valid, X_test): # training %\n",
    "\n",
    "  trainig_dataset_path = 'Preprocessed datasets/pre-training/training/'\n",
    "  validation_dataset_path = 'Preprocessed datasets/pre-training/validation/'\n",
    "  testing_dataset_path = 'Preprocessed datasets/pre-training/testing/'\n",
    "\n",
    "  try:\n",
    "    mlm_ds = tf.data.experimental.load(trainig_dataset_path)\n",
    "    valid_ds = tf.data.experimental.load(validation_dataset_path)\n",
    "    test_ds = tf.data.experimental.load(testing_dataset_path)\n",
    "    print('preprocessed datasets already existed')\n",
    "\n",
    "  except:\n",
    "    print('preprocessed datasets not found')\n",
    "\n",
    "    # Prepare data for pre-trained model \n",
    "    x_all_raw = tf.data.Dataset.from_tensor_slices(\n",
    "                tf.cast(X_train.code.values, tf.string)) \n",
    "\n",
    "    x_all_code = x_all_raw.map(encode, \n",
    "                  num_parallel_calls=config.BUFFER_SIZE)\n",
    "\n",
    "    x_valid_raw = tf.data.Dataset.from_tensor_slices(\n",
    "                tf.cast(X_valid.code.values, tf.string)) \n",
    "\n",
    "    x_valid_code = x_valid_raw.map(encode, \n",
    "                  num_parallel_calls=config.BUFFER_SIZE)\n",
    "    \n",
    "    x_test_raw = tf.data.Dataset.from_tensor_slices(\n",
    "                tf.cast(X_test.code.values, tf.string)) \n",
    "\n",
    "    x_test_code = x_test_raw.map(encode, \n",
    "                  num_parallel_calls=config.BUFFER_SIZE)\n",
    "    \n",
    "    # Applying the Proxytask on training ds\n",
    "    a = tfds.as_numpy(x_all_code)\n",
    "    x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(a)\n",
    "\n",
    "    mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      ( x_masked_train, y_masked_labels, sample_weights)\n",
    "      )\n",
    "\n",
    "    mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE).prefetch(config.BUFFER_SIZE)\n",
    "\n",
    "    # Applying the Proxytask on validation and testing datasets\n",
    "    x_masked_val, y_masked_labels_val, sample_weights_val = get_masked_input_and_labels(tfds.as_numpy(x_valid_code))\n",
    "\n",
    "    valid_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      (x_masked_val, y_masked_labels_val, sample_weights_val)\n",
    "      )\n",
    "\n",
    "    valid_ds = valid_ds.shuffle(1000).batch(config.BATCH_SIZE).prefetch(config.BUFFER_SIZE)\n",
    "    \n",
    "    x_masked_test, y_masked_labels_test, sample_weights_test = get_masked_input_and_labels(tfds.as_numpy(x_test_code))\n",
    "\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      (x_masked_test, y_masked_labels_test, sample_weights_test)\n",
    "      )\n",
    "    \n",
    "    test_ds = test_ds.shuffle(1000).batch(config.BATCH_SIZE).prefetch(config.BUFFER_SIZE)\n",
    "    \n",
    "    # Save datasets\n",
    "    tf.data.experimental.save(mlm_ds, trainig_dataset_path)\n",
    "    tf.data.experimental.save(valid_ds, validation_dataset_path)\n",
    "    tf.data.experimental.save(test_ds, testing_dataset_path)\n",
    "\n",
    "\n",
    "  return mlm_ds, valid_ds, test_ds, #x_masked_val, y_masked_labels_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j54OK88Nq13o"
   },
   "source": [
    "## Create DL model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ouEZjmnv9Pg"
   },
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        temp = []\n",
    "        for t in tokens:\n",
    "           if t != 0 and t < len(id2token):\n",
    "             temp.append(id2token[t])\n",
    "        return \" \".join(temp)\n",
    "\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        if id >= len(id2token):\n",
    "          return ''\n",
    "        else:\n",
    "          return id2token[id]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        masked_index = np.where(self.sample_tokens == MASK_TOKEN_ID)\n",
    "        masked_index = masked_index[1]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            #print(tokens)\n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68p6whyEyJsE"
   },
   "outputs": [],
   "source": [
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
    "            if pos != 0\n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return pos_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcfQ4A2MybT8"
   },
   "source": [
    "### ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTNogFQTyezF"
   },
   "outputs": [],
   "source": [
    "def create_dl_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "    #x_in = layers.Dense(256)(inputs) \n",
    "    embeddings = layers.Embedding(config.VOCAB_SIZE, config.EMBED_DIM, name=\"embedding\" )(inputs)\n",
    "    \n",
    "    L4 = embeddings\n",
    "\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "      L1 = layers.Dense(config.FF_DIM, activation=\"relu\")(L4) \n",
    "      L3 = layers.Dropout(rate = config.DROPOUT, )(L1) \n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(L3)\n",
    "\n",
    "    mlm_model = keras.Model(inputs, mlm_output, name=\"dl_model\")  \n",
    "\n",
    "    return mlm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJ6iHBFg0axL"
   },
   "source": [
    "### CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxjqOncQ0dPh"
   },
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "    embeddings = layers.Embedding(config.VOCAB_SIZE, config.EMBED_DIM, name=\"embedding\" )(inputs)\n",
    "\n",
    "    L2 = embeddings\n",
    "\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "      L1 = Conv1D(filters = config.FF_DIM, kernel_size =3, padding = 'SAME')(L2)\n",
    "      L2 = layers.ReLU()(L1) \n",
    "    #L3 = layers.MaxPool1D( padding = 'SAME')(L2)\n",
    "\n",
    "    #L4 = layers.Flatten()(L3)\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(L2)\n",
    "\n",
    "    mlm_model = keras.Model(inputs, mlm_output, name=\"cnn_model\")\n",
    "\n",
    "    #optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    #mlm_model.compile(optimizer=optimizer)\n",
    "\n",
    "    return mlm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POcvUnIG2aj0"
   },
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTsb2Jer2deL"
   },
   "outputs": [],
   "source": [
    "def create_lstm_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "    #x_in = layers.Dense(256)(inputs) \n",
    "    embeddings = layers.Embedding(config.VOCAB_SIZE, config.EMBED_DIM, name=\"embedding\" )(inputs)\n",
    "\n",
    "    L2 = embeddings\n",
    "\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "      L1 = LSTM(config.FF_DIM, return_sequences= True)(L2)\n",
    "\n",
    "      L2 = layers.Dropout(rate = config.DROPOUT, )(L1) \n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(L2)\n",
    "\n",
    "    mlm_model = keras.Model(inputs, mlm_output, name=\"lstm_model\")\n",
    "\n",
    "    #optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    #mlm_model.compile(optimizer=optimizer)\n",
    "    \n",
    "    return mlm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6VrMWDJyTpp"
   },
   "source": [
    "### Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vBvo9mRq2w8"
   },
   "outputs": [],
   "source": [
    "def transformer_model():\n",
    "\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "    word_embeddings = layers.Embedding(config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\")(inputs)\n",
    "    \n",
    "    position_embeddings = layers.Embedding(\n",
    "        input_dim=config.MAX_LEN,\n",
    "        output_dim=config.EMBED_DIM,\n",
    "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
    "        name=\"position_embedding\",)(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
    "    \n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    encoder_output = embeddings\n",
    "\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "      encoder_output = transformer_Encoder(encoder_output, encoder_output, encoder_output, i)\n",
    "      decoder_output = transformer_Decoder(embeddings, embeddings, embeddings, encoder_output, i)\n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(decoder_output)\n",
    "    \n",
    "    mlm_model = keras.Model(inputs, mlm_output, name=\"transformer_model\")\n",
    "\n",
    "\n",
    "    return mlm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cSnpx9sySo-"
   },
   "outputs": [],
   "source": [
    "def transformer_Encoder (query, key, value, i):\n",
    "\n",
    "    # Multi headed self-attention\n",
    "    attention_output = layers.MultiHeadAttention(num_heads=config.NUM_HEAD, key_dim=config.EMBED_DIM // config.NUM_HEAD,)(query, key, value)\n",
    "    \n",
    "    attention_output = layers.Dropout(config.DROPOUT)(attention_output)\n",
    "    \n",
    "    attention_output = layers.LayerNormalization(epsilon=1e-6,)(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ], name=\"encoder_{}/ffn\".format(i),)\n",
    "    \n",
    "    ffn_output = ffn(attention_output)\n",
    "    \n",
    "    ffn_output = layers.Dropout(config.DROPOUT)(ffn_output)\n",
    "    \n",
    "    sequence_output = layers.LayerNormalization(epsilon=1e-6)(attention_output + ffn_output)\n",
    "    \n",
    "    return sequence_output\n",
    "\n",
    "def transformer_Decoder (query, key, value, enc_output, i):\n",
    "\n",
    "    # Multi headed self-attention 1\n",
    "    attention_output = layers.MultiHeadAttention(num_heads=config.NUM_HEAD, key_dim=config.EMBED_DIM // config.NUM_HEAD,)(query, key, value)\n",
    "    \n",
    "    attention_output = layers.Dropout(config.DROPOUT)(attention_output)\n",
    "    \n",
    "    attention_output = layers.LayerNormalization(epsilon=1e-6,)(query + attention_output)\n",
    "\n",
    "    # Multi headed self-attention 2\n",
    "    attention_output2 = layers.MultiHeadAttention(num_heads=config.NUM_HEAD, key_dim=config.EMBED_DIM // config.NUM_HEAD,)(attention_output, enc_output, enc_output)\n",
    "    \n",
    "    attention_output2 = layers.Dropout(config.DROPOUT)(attention_output2)\n",
    "    \n",
    "    attention_output2 = layers.LayerNormalization(epsilon=1e-6,)(attention_output + attention_output2)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ], name=\"decoder_{}/ffn\".format(i),)\n",
    "    \n",
    "    ffn_output = ffn(attention_output2)\n",
    "    \n",
    "    ffn_output = layers.Dropout(config.DROPOUT)(ffn_output)\n",
    "    \n",
    "    sequence_output = layers.LayerNormalization(epsilon=1e-6)(attention_output2 + ffn_output)\n",
    "    \n",
    "    return sequence_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXpPfNDMztMB"
   },
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuJN_wrFzZBl"
   },
   "outputs": [],
   "source": [
    "def pre_train_model(train_ds, valid_ds, epochs, model_name, callbacks, config, logs):\n",
    "  tf.get_logger().setLevel('ERROR')\n",
    "  strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "\n",
    "  with strategy.scope():\n",
    "    model = get_model(model_name)\n",
    "  \n",
    "  History = model.fit(train_ds, epochs=epochs, validation_data = valid_ds, callbacks=callbacks)\n",
    "\n",
    "  eval_results = model.evaluate(valid_ds, verbose ='0', batch_size=config.BATCH_SIZE)\n",
    "  loss = eval_results[0] \n",
    "  accuracy = eval_results[1] \n",
    "  \n",
    "  model.save(logs + \"/\" + model.name + \".h5\")\n",
    "  config.save(logs + \"/\" + model.name + \"_config.txt\")\n",
    "\n",
    "  return accuracy, loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_W0U_KxgmlpZ"
   },
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "\n",
    "\n",
    "  if model_name == 'DL': model = create_dl_model()\n",
    "  if model_name == 'LSTM': model = create_lstm_model()\n",
    "  if model_name == 'CNN': model = create_cnn_model()\n",
    "  if model_name == 'Transformer': model = transformer_model()\n",
    "\n",
    "  optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "  loss =  keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "  model.compile(\n",
    "        optimizer= optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        weighted_metrics=[\"sparse_categorical_accuracy\"], #jit_compile=True,\n",
    "  )\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjj5YA9yPvUw"
   },
   "outputs": [],
   "source": [
    "def pre_processing(data, _filtering = True, _truncation= True):\n",
    "  new_data = data[0:][['code', 'label']]\n",
    "  \n",
    "  if _drive: \n",
    "    new_data = data[0:150][['code', 'label']]\n",
    "  \n",
    "  new_data['code'] = new_data['code'].astype(str)\n",
    "\n",
    "  if _filtering:\n",
    "    new_data = data_filter(new_data)\n",
    "\n",
    "  if _truncation: \n",
    "    new_data = truncation(new_data)\n",
    "\n",
    "  return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1urVoA7Vqw-"
   },
   "outputs": [],
   "source": [
    "def get_callbacks(sample_tokens, log_dir):\n",
    "  #sample_tokens = vectorize_layer([\"[mask] processJarAttrs ( ) throws BuildException\" ])\n",
    "  #sample_tokens = vectorize_layer([\"[mask] index = 1\" ])\n",
    "  generator_callback = TextGenerator(sample_tokens.numpy())\n",
    "  \n",
    "  tboard_callback = keras.callbacks.TensorBoard(log_dir = log_dir,\n",
    "                                                  histogram_freq = 1,\n",
    "                                                  profile_batch = '20,31')\n",
    "  \n",
    "  earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 5, \n",
    "                                        restore_best_weights = True, verbose=1)\n",
    "  #Hparam_callback = hp.KerasCallback(logdir, hparams)\n",
    "\n",
    "  return [generator_callback, tboard_callback, earlystopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmwrCQkK4aOW"
   },
   "outputs": [],
   "source": [
    "def split_data(data, training_ratio= 0.8, _valid=False):\n",
    "\n",
    "  train_size=0.8\n",
    "\n",
    "  X = data.drop(columns = ['label']).copy()\n",
    "  y = data['label']\n",
    "\n",
    "  X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=training_ratio)\n",
    "\n",
    "  if  _valid == False:\n",
    "    return X_train, X_rem, y_train, y_rem\n",
    "  \n",
    "  else:\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
    "    return  X_train, X_valid, X_test, y_train, y_valid, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNtwLjdC102C"
   },
   "source": [
    "## ----- Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7RxJMskusRT"
   },
   "outputs": [],
   "source": [
    "dataset = load_data_from_file('Data/All projects.csv')\n",
    "\n",
    "keywords = get_keywords('Java-keywords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLajVcR_f5gF"
   },
   "outputs": [],
   "source": [
    "all_data = pre_processing(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpQFxC6kyKhT"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test ,_,_,_ = split_data(all_data, training_ratio= 0.8, _valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XFSZEuwdsY5"
   },
   "outputs": [],
   "source": [
    "#!rm -rf ./vec_layer_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8SaEWxOY-wR"
   },
   "outputs": [],
   "source": [
    "# Get vec layer for tokenization\n",
    "vec_layer_logs = 'vec_layer_logs/'+''+'tv_layer.pkl'\n",
    "\n",
    "vectorize_layer = get_vectorize_layer(keywords+\n",
    "    X_train.code.values.tolist(),\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN, \n",
    "    vec_log= vec_layer_logs,\n",
    "    special_tokens= ['[mask]'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30qqv3nu0pqh"
   },
   "outputs": [],
   "source": [
    "# Get mask token id for masked language model\n",
    "MASK_TOKEN_ID = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
    "print('--------------', MASK_TOKEN_ID)\n",
    "\n",
    "# Get dictionary of keywords\n",
    "keywords_dic = {keyword: encode(keyword).numpy().tolist()[0] for keyword in keywords} #keywords_dic[keywords[1]]\n",
    "\n",
    "# Get dictionary of tokens ids\n",
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))  #id2token[1]\n",
    "token2id = {y: x for x, y in id2token.items()}  #token2id['[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uw3EYnduxpUS"
   },
   "outputs": [],
   "source": [
    "mlm_train_ds, mlm_valid_ds, mlm_test_ds= get_pretraining_dataset(X_train, X_valid, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPD97bS7SQV8"
   },
   "outputs": [],
   "source": [
    "config.CODE_VERSION = 'v7'\n",
    "MODEL_NAME = 'Transformer' # DL, LSTM, CNN, Transformer\n",
    "logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")+ '_' + MODEL_NAME\n",
    "\n",
    "sample_tokens = vectorize_layer([\"[mask] index = 1\" ])\n",
    "\n",
    "callbacks= get_callbacks(sample_tokens, logs) \n",
    "\n",
    "model = get_model(MODEL_NAME)\n",
    "\n",
    "_,_, model= pre_train_model(mlm_train_ds, mlm_valid_ds, 5, MODEL_NAME, callbacks, config, logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5n6n0lktSQV8"
   },
   "outputs": [],
   "source": [
    "def decode(tokens):\n",
    "    temp = []\n",
    "    for t in tokens:\n",
    "        if t != 0 and t < len(id2token):\n",
    "            temp.append(id2token[t])\n",
    "    return \" \".join(temp)\n",
    "\n",
    "def convert_ids_to_tokens(id):\n",
    "    if id >= len(id2token):\n",
    "        return ''\n",
    "    else:\n",
    "        return id2token[id]\n",
    "\n",
    "def get_predictions(prediction, k):\n",
    "  masked_index = np.where(test_tokens == MASK_TOKEN_ID)\n",
    "  masked_index = masked_index[1]\n",
    "  mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "  top_indices = mask_prediction[0].argsort()[-k:][::-1]\n",
    "  values = mask_prediction[0][top_indices]\n",
    "\n",
    "  for i in range(len(top_indices)):\n",
    "      p = top_indices[i]\n",
    "      v = values[i]\n",
    "      tokens = np.copy(test_tokens[0])\n",
    "      tokens[masked_index[0]] = p\n",
    "              #print(tokens)\n",
    "      result = {\n",
    "          \"input_text\": decode(test_tokens[0].numpy()),\n",
    "          \"prediction\": decode(tokens),\n",
    "          \"probability\": v,\n",
    "          \"predicted mask token\": convert_ids_to_tokens(p),\n",
    "      }\n",
    "      pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = vectorize_layer([\"[mask] text = ' some text '\" ])\n",
    "prediction = model.predict(test_tokens)\n",
    "top_k =3\n",
    "\n",
    "get_predictions(prediction, top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN0v09R8pqB0"
   },
   "source": [
    "## Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUn_UJuameo3"
   },
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "#!rm -rf ./logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yDDDDEvSQV8"
   },
   "outputs": [],
   "source": [
    "def check_existed_params(df, params):\n",
    "\n",
    "  if MODEL_NAME == 'Transformer':\n",
    "      existed_params = df.loc[(df['state'] == 'COMPLETE') & (df['params_Learning rate'] == params['Learning rate']) & \n",
    "                          (df['params_Dropout rate'] == params['Dropout rate'])& \n",
    "                          (df['params_Batch size'] == params['Batch size'])& \n",
    "                          (df['params_Num. layers'] == params['Num. layers'])& \n",
    "                          (df['params_Layers Dim'] == params['Layers Dim'])& \n",
    "                          (df['params_Emmbedding Dim.'] == params['Emmbedding Dim.'])& \n",
    "                          (df['params_Num. heads'] == params['Num. heads'])] \n",
    "  else:\n",
    "      existed_params = df.loc[(df['state'] == 'COMPLETE') &  (df['params_Learning rate'] == params['Learning rate']) & \n",
    "                          (df['params_Dropout rate'] == params['Dropout rate'])& \n",
    "                          (df['params_Batch size'] == params['Batch size'])& \n",
    "                          (df['params_Num. layers'] == params['Num. layers'])& \n",
    "                          (df['params_Layers Dim'] == params['Layers Dim'])& \n",
    "                          (df['params_Emmbedding Dim.'] == params['Emmbedding Dim.'])\n",
    "                          ] \n",
    "  \n",
    "  print('====== existed_params ', existed_params)\n",
    "  \n",
    "  return not existed_params.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oK1dU5qfwryo"
   },
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    \n",
    "    log_dir =  run_dir.replace('hparam_tuning/','')\n",
    "    tboard_callback = keras.callbacks.TensorBoard(log_dir = log_dir,\n",
    "                                                  histogram_freq = 1,\n",
    "                                                  profile_batch = '20,31')\n",
    "    \n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "\n",
    "        start = time.time()\n",
    "        accuracy, loss, _ = pre_train_model(mlm_train_ds, mlm_valid_ds, 5, MODEL_NAME, tboard_callback, config, run_dir)\n",
    "        training_time = round((time.time() - start)/60)\n",
    "\n",
    "        tf.summary.scalar('Accuracy', accuracy, step=1)\n",
    "        tf.summary.scalar('Loss', loss, step=1)\n",
    "        tf.summary.scalar('Time (Min.)', training_time, step=1)\n",
    "\n",
    "        return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQHlxW2cbE2D"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    #model_name = trial.suggest_categorical('model', ['DL', 'CNN', 'LSTM', 'Transformer'])\n",
    "    lr = trial.suggest_categorical('Learning rate', [0.0001, 0.001, 0.01])\n",
    "    dropout_rate = trial.suggest_categorical('Dropout rate', [0.1, 0.2, 0.3])\n",
    "    batch_size = trial.suggest_categorical('Batch size', [64, 128, 256])\n",
    "    num_layers = trial.suggest_categorical('Num. layers', [3, 6, 12])\n",
    "    num_units = trial.suggest_categorical('Layers Dim', [512, 1024, 2048])\n",
    "    emb_dim = trial.suggest_categorical('Emmbedding Dim.', [128, 256, 512])\n",
    "    \n",
    "    \n",
    "    hparams = {\n",
    "                  #'model': model_name,\n",
    "                  'Learning rate': lr,\n",
    "                  'Dropout rate': dropout_rate,\n",
    "                  'Batch size': batch_size,\n",
    "                  'Num. layers': num_layers,\n",
    "                  'Layers Dim': num_units,\n",
    "                  'Emmbedding Dim.': emb_dim,\n",
    "\n",
    "              }\n",
    "\n",
    "    config.BATCH_SIZE = batch_size\n",
    "    config.NUM_LAYERS = num_layers\n",
    "    config.FF_DIM = num_units\n",
    "    config.EMBED_DIM = emb_dim\n",
    "    config.LR = lr\n",
    "\n",
    "    if MODEL_NAME == 'Transformer':\n",
    "      num_heads = trial.suggest_categorical('Num. heads', [8, 16])\n",
    "      hparams['Num. heads'] = num_heads\n",
    "      config.NUM_HEAD = num_heads\n",
    "\n",
    "        \n",
    "    df = study.trials_dataframe()\n",
    "    try:\n",
    "      if check_existed_params(df, hparams): #not existed_params.empty:\n",
    "        print('================== param already existed')\n",
    "        return 100 #study.trials_dataframe().value[a['number']][1]\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "    run_name = \"run-%d\" % trial.number\n",
    "    accuracy, loss = run(hp_logs + '/' + run_name, hparams)\n",
    "    trial.set_user_attr(\"accuracy\", accuracy)\n",
    "\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2m3wH97YXw7j",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config.CODE_VERSION = 'DL_model_v7_HPOpt_Parall_optuna_eval'\n",
    "MODEL_NAME = 'Transformer' # DL, LSTM, CNN, Transformer\n",
    "hp_logs = 'logs/hparam_tuning/' + MODEL_NAME+ datetime.now().strftime(\" %d-%m-%Y\")\n",
    "optuna_logs = \"logs/optuna/\" + MODEL_NAME+ datetime.now().strftime(\" %d-%m-%Y\")\n",
    "\n",
    "study_name = MODEL_NAME # Unique identifier of the study.\n",
    "storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "\n",
    "\n",
    "study = optuna.create_study(study_name= MODEL_NAME, \n",
    "                            direction='minimize', \n",
    "                            storage =storage_name, \n",
    "                            pruner=optuna.pruners.MedianPruner(),\n",
    "                            load_if_exists =True) #maximize val accuracy or min val loss\n",
    "\n",
    "study.optimize(objective, n_trials=25)\n",
    "\n",
    "pickle.dump(study, open(optuna_logs +\".pkl\", \"wb\"))\n",
    "study.trials_dataframe().to_csv(optuna_logs + '.csv')\n",
    "\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Loss: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "lxj_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
